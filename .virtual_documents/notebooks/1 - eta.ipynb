


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


# Load the datasets
sales_data = pd.read_csv('../data/snp_dld_2024_transactions.csv', low_memory=False)
rentals_data = pd.read_csv('../data/snp_dld_2024_rents.csv', low_memory=False)


# Display basic info
print("Sales Data Info:")
print(sales_data.info())
print("\nRentals Data Info:")
print(rentals_data.info())





# %%
# List of columns to drop from sales_data
columns_to_drop_sales = [
    'transaction_number',  # ID column
    'entry_id',            # Metadata
    'meta_ts',             # Metadata timestamp
    'master_project_en',   # Very few non-null values
    'master_project_ar',   # Very few non-null values
    'property_type_ar',    # Duplicate of property_type_en
    'property_subtype_ar', # Duplicate of property_subtype_en
    'rooms_ar',            # Duplicate of rooms_en
    'project_name_ar',     # Duplicate of project_name_en
    'area_ar',             # Duplicate of area_en
    'nearest_landmark_ar', # Duplicate of nearest_landmark_en
    'nearest_metro_ar',    # Duplicate of nearest_metro_en
    'nearest_mall_ar'      # Duplicate of nearest_mall_en
]

# Drop columns from sales_data
sales_data.drop(columns=columns_to_drop_sales, inplace=True, errors='ignore')

# List of columns to drop from rentals_data
columns_to_drop_rentals = [
    'ejari_contract_number', # ID column
    'land_property_id',      # Redundant ID column
    'entry_id',              # Metadata
    'meta_ts',               # Metadata timestamp
    'master_project_en',     # Very few non-null values
    'master_project_ar',     # Very few non-null values
    'property_type_ar',      # Duplicate of property_type_en
    'property_subtype_ar',   # Duplicate of property_subtype_en
    'property_usage_ar',     # Duplicate of property_usage_en
    'project_name_ar',       # Duplicate of project_name_en
    'area_ar',               # Duplicate of area_en
    'nearest_landmark_ar',   # Duplicate of nearest_landmark_en
    'nearest_metro_ar',      # Duplicate of nearest_metro_en
    'nearest_mall_ar'        # Duplicate of nearest_mall_en
]

# Drop columns from rentals_data
rentals_data.drop(columns=columns_to_drop_rentals, inplace=True, errors='ignore')

# Verify remaining columns
print("Remaining columns in sales_data:")
print(sales_data.columns)
print("\nRemaining columns in rentals_data:")
print(rentals_data.columns)






def find_constant_columns_with_details(df, dataset_name):
    """
    Identifies columns in a DataFrame that have only one unique value or are entirely NaN.
    Prints detailed results for the given dataset.
    """
    constant_columns = []
    for col in df.columns:
        unique_values = df[col].unique()
        if len(unique_values) == 1 or df[col].isnull().all():
            constant_columns.append(col)
            print(f"{col}: {len(unique_values)} unique values, Values: {unique_values}")
    if constant_columns:
        print(f"\nConstant columns in {dataset_name}: {constant_columns}")
    else:
        print(f"\nNo constant columns found in {dataset_name}.")
    return constant_columns

sales_constant_columns = find_constant_columns_with_details(sales_data, "Sales Data")




rentals_constant_columns = find_constant_columns_with_details(rentals_data, "Rentals Data")



# Drop constant columns from sales_data
sales_data.drop(columns=sales_constant_columns, inplace=True)
print(f"Dropped constant columns from Sales Data: {sales_constant_columns}")

# Drop constant columns from rentals_data
rentals_data.drop(columns=rentals_constant_columns, inplace=True)
print(f"Dropped constant columns from Rentals Data: {rentals_constant_columns}")



# Count the number of columns in sales_data
sales_columns_count = sales_data.shape[1]
print(f"Number of columns in Sales Data: {sales_columns_count}")

# Count the number of columns in rentals_data
rentals_columns_count = rentals_data.shape[1]
print(f"Number of columns in Rentals Data: {rentals_columns_count}")






# Duplicate of transaction_type_id
columns_to_drop = ['transaction_type_en']

sales_data.drop(columns=columns_to_drop, inplace=True)




# Duplicate of property_usage_id

columns_to_drop = ['property_usage_en']

sales_data.drop(columns=columns_to_drop, inplace=True)


print("Updated Sales Data:")
print(sales_data.info())


sales_data.columns


def find_all_overlapping_columns(df):
    """
    Find all overlapping column pairs in a DataFrame where one column's values 
    uniquely map to another column's values and validate that the mappings align.

    Parameters:
        df (DataFrame): The dataset to analyze.

    Returns:
        List[Tuple[str, str]]: List of tuples with all overlapping column pairs.
    """
    overlapping_pairs = []
    for reference_column in df.columns:
        for col in df.columns:
            if col != reference_column:
                # Check if the reference column uniquely maps to the current column
                mapping_valid = (
                    df.groupby(reference_column)[col].nunique().nunique() == 1
                )
                # Additional check: Ensure columns have identical values for valid mappings
                if mapping_valid and (df[reference_column].astype(str) == df[col].astype(str)).all():
                    overlapping_pairs.append((reference_column, col))
    return overlapping_pairs

# Apply the function to sales_data
sales_overlapping_pairs = find_all_overlapping_columns(sales_data)

# Print the results
print("Validated Overlapping Column Pairs in Sales Data:")
print(sales_overlapping_pairs)




# Function to display the head of overlapping column pairs
def print_overlapping_pairs_head(df, overlapping_pairs):
    """
    Print the first few rows (head) of overlapping column pairs for inspection.

    Parameters:
        df (DataFrame): The dataset to analyze.
        overlapping_pairs (List[Tuple[str, str]]): List of column pairs to inspect.
    """
    for col1, col2 in overlapping_pairs:
        print(f"Head of {col1} and {col2}:")
        print(df[[col1, col2]].head())
        print("\n")

# Apply the overlapping analysis function to sales_data
sales_overlapping_pairs = find_all_overlapping_columns(sales_data)

# Print the head of each overlapping column pair
print_overlapping_pairs_head(sales_data, sales_overlapping_pairs)



print("Updated Sales Data:")
print(rentals_data.info())


# Apply the overlapping analysis function to sales_data
rentals_overlapping_pairs = find_all_overlapping_columns(rentals_data)

# Print the head of each overlapping column pair
print_overlapping_pairs_head(rentals_data, rentals_overlapping_pairs)


# Inspect column data types
print(sales_data.dtypes)




# Inspect column data types
print(rentals_data.dtypes)





# Identify date columns
sales_date_columns = ['transaction_datetime', 'req_from', 'req_to']
rentals_date_columns = ['registration_date', 'contract_start_date', 'contract_end_date', 'req_from', 'req_to']
# Function to handle dates: convert to datetime and extract features
def handle_date_columns(df, date_columns):
    """
    Handles date columns by converting them to datetime and extracting features.
    
    Parameters:
        df (DataFrame): The DataFrame to process.
        date_columns (list): List of column names to handle.
    
    Returns:
        DataFrame: Updated DataFrame with extracted date features.
    """
    for col in date_columns:
        # Convert to datetime
        df[col] = pd.to_datetime(df[col], errors='coerce')
        
        # Extract features
        df[f"{col}_year"] = df[col].dt.year
        df[f"{col}_month"] = df[col].dt.month
        df[f"{col}_day"] = df[col].dt.day
        df[f"{col}_weekday"] = df[col].dt.weekday  # 0 = Monday, 6 = Sunday
        df[f"{col}_dayofyear"] = df[col].dt.dayofyear
    
    # Optionally drop the original date columns
    df.drop(columns=date_columns, inplace=True)
    
    return df

# Apply the function to sales_data and rentals_data
sales_data = handle_date_columns(sales_data, sales_date_columns)
rentals_data = handle_date_columns(rentals_data, rentals_date_columns)

# Display results
print("Sales Data after date handling:")
print(sales_data.info())

print("\nRentals Data after date handling:")
print(rentals_data.info())





def classify_columns_exclude_processed(df, exclude_columns=[]):
    """
    Classifies columns in a DataFrame as boolean or categorical, excluding specified columns.
    
    Parameters:
        df (DataFrame): The DataFrame to analyze.
        exclude_columns (list): List of columns to exclude from analysis.
    
    Returns:
        Dict: Dictionary with lists of boolean and categorical columns.
    """
    boolean_columns = []
    categorical_columns = []
    
    for col in df.columns:
        if col in exclude_columns:
            continue  # Skip excluded columns
        
        unique_values = df[col].nunique()
        dtype = df[col].dtype
        
        # Boolean detection: bool type or exactly two unique values
        if dtype == 'bool' or (unique_values == 2 and dtype in ['object', 'int64', 'float64']):
            boolean_columns.append(col)
        # Categorical detection: object or category type, or limited unique values
        elif dtype in ['object', 'category'] or unique_values <= 20:
            categorical_columns.append(col)
    
    return {
        "boolean_columns": boolean_columns,
        "categorical_columns": categorical_columns
    }

# Example Usage
# Exclude columns already processed, like datetime-derived features
exclude_columns_sales = [col for col in sales_data.columns if col.endswith(('_year', '_month', '_day', '_weekday', '_dayofyear'))]
exclude_columns_rentals = [col for col in rentals_data.columns if col.endswith(('_year', '_month', '_day', '_weekday', '_dayofyear'))]

sales_column_classes = classify_columns_exclude_processed(sales_data, exclude_columns=exclude_columns_sales)
rentals_column_classes = classify_columns_exclude_processed(rentals_data, exclude_columns=exclude_columns_rentals)

print("Sales Data Column Classes (excluding processed features):", sales_column_classes)
print("Rentals Data Column Classes (excluding processed features):", rentals_column_classes)




# Updated Rentals Column Classes
rentals_column_classes['boolean_columns'].remove('version_text')
rentals_column_classes['categorical_columns'].append('version_text')

# Function to process boolean columns (unchanged)
def process_boolean_columns(df, boolean_columns):
    """
    Processes boolean columns by mapping their values to 0 and 1.
    """
    for col in boolean_columns:
        df[col] = df[col].map({'t': 1, 'f': 0, 'True': 1, 'False': 0, True: 1, False: 0})
    return df

# Function to process categorical columns (unchanged)
from sklearn.preprocessing import LabelEncoder

def process_categorical_columns(df, categorical_columns, encoding='label'):
    """
    Processes categorical columns by encoding their values.
    """
    if encoding == 'label':
        # Label Encoding
        for col in categorical_columns:
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col].astype(str))
    elif encoding == 'one-hot':
        # One-Hot Encoding
        df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)
    
    return df

# Process boolean columns for Sales Data
sales_data = process_boolean_columns(sales_data, sales_column_classes['boolean_columns'])

# Process boolean columns for Rentals Data
rentals_data = process_boolean_columns(rentals_data, rentals_column_classes['boolean_columns'])

# Process categorical columns for Sales Data
sales_data = process_categorical_columns(sales_data, sales_column_classes['categorical_columns'], encoding='label')

# Process categorical columns for Rentals Data
rentals_data = process_categorical_columns(rentals_data, rentals_column_classes['categorical_columns'], encoding='label')

# Display results
print("Processed Sales Data:")
print(sales_data.info())

print("\nProcessed Rentals Data:")
print(rentals_data.info())






# Display the number and percentage of missing values for Sales Data
print("Missing Values in Sales Data:")
missing_sales = sales_data.isnull().sum().to_frame(name="Missing Count")
missing_sales["Percentage"] = (missing_sales["Missing Count"] / len(sales_data)) * 100
print(missing_sales[missing_sales["Missing Count"] > 0])

# Display the number and percentage of missing values for Rentals Data
print("\nMissing Values in Rentals Data:")
missing_rentals = rentals_data.isnull().sum().to_frame(name="Missing Count")
missing_rentals["Percentage"] = (missing_rentals["Missing Count"] / len(rentals_data)) * 100
print(missing_rentals[missing_rentals["Missing Count"] > 0])






# Drop 'is_freehold_text' from Sales Data and Rentals Data
sales_data.drop(columns=['is_freehold_text'], inplace=True)
rentals_data.drop(columns=['is_freehold_text'], inplace=True)





# Count missing values and unique values in 'parcel_id'
print("Sales Data:")
print(f"Missing values in 'parcel_id': {sales_data['parcel_id'].isnull().sum()}")
print(f"Unique values in 'parcel_id': {sales_data['parcel_id'].nunique()}")
print(f"Total rows in Sales Data: {len(sales_data)}")

print("\nRentals Data:")
print(f"Missing values in 'parcel_id': {rentals_data['parcel_id'].isnull().sum()}")
print(f"Unique values in 'parcel_id': {rentals_data['parcel_id'].nunique()}")
print(f"Total rows in Rentals Data: {len(rentals_data)}")






# Sales Data: Correlation with 'amount'
if 'parcel_id' in sales_data.columns:
    sales_correlation = sales_data[['parcel_id', 'amount']].dropna().corr()
    print("\nCorrelation of 'parcel_id' with 'amount' in Sales Data:")
    print(sales_correlation)

# Rentals Data: Correlation with 'annual_amount'
if 'parcel_id' in rentals_data.columns:
    rentals_correlation = rentals_data[['parcel_id', 'annual_amount']].dropna().corr()
    print("\nCorrelation of 'parcel_id' with 'annual_amount' in Rentals Data:")
    print(rentals_correlation)






# Drop 'parcel_id' from both datasets
sales_data.drop(columns=['parcel_id'], inplace=True)
rentals_data.drop(columns=['parcel_id'], inplace=True)





# Drop 'parking' from the rentals dataset
rentals_data.drop(columns=['parking'], inplace=True)


# Check unique values and missing counts before processing
print("Unique values in 'registration_type_en':", sales_data['registration_type_en'].unique())
print("Missing values in 'registration_type_en':", sales_data['registration_type_en'].isnull().sum())

print("\nUnique values in 'is_freehold_text':", sales_data['is_freehold_text'].unique())
print("Missing values in 'is_freehold_text':", sales_data['is_freehold_text'].isnull().sum())






sales_data_original = pd.read_csv('../data/snp_dld_2024_transactions.csv', low_memory=False)


# Verify the column of interest
print("Original Unique Values in 'registration_type_en':")
print(sales_data_original['registration_type_en'].unique())


# Check unique values and missing values
print("Unique values in 'registration_type_en':")
print(sales_data_original['registration_type_en'].unique())

print("\nMissing values in 'registration_type_en':")
print(sales_data_original['registration_type_en'].isnull().sum())



# Binary Encoding
sales_data_original['registration_type_en_encoded'] = sales_data_original['registration_type_en'].map({'Off-Plan': 0, 'Ready': 1})

# Verify encoding
print("Binary Encoded 'registration_type_en':")
print(sales_data_original[['registration_type_en', 'registration_type_en_encoded']].head())



# Update the 'registration_type_en' in sales_data with the encoded column from sales_data_original
sales_data['registration_type_en'] = sales_data_original['registration_type_en_encoded']

# Verify the updated column
print("Updated 'registration_type_en' in sales_data:")
print(sales_data[['registration_type_en']].head())



# Display the number and percentage of missing values for Sales Data
print("Missing Values in Sales Data:")
missing_sales = sales_data.isnull().sum().to_frame(name="Missing Count")
missing_sales["Percentage"] = (missing_sales["Missing Count"] / len(sales_data)) * 100
print(missing_sales[missing_sales["Missing Count"] > 0])

# Display the number and percentage of missing values for Rentals Data
print("\nMissing Values in Rentals Data:")
missing_rentals = rentals_data.isnull().sum().to_frame(name="Missing Count")
missing_rentals["Percentage"] = (missing_rentals["Missing Count"] / len(rentals_data)) * 100
print(missing_rentals[missing_rentals["Missing Count"] > 0])





# Sales Data: Handle missing values
sales_data['property_usage_id'].fillna(sales_data['property_usage_id'].mode()[0], inplace=True)  # Fill with mode
sales_data['transaction_size_sqm'].fillna(sales_data['transaction_size_sqm'].median(), inplace=True)  # Fill with median
sales_data['is_freehold'].fillna(sales_data['is_freehold'].mode()[0], inplace=True)  # Fill with mode

# Rentals Data: Handle missing values
rentals_data['annual_amount'].fillna(rentals_data['annual_amount'].median(), inplace=True)  # Fill with median

# Verify that all missing values are handled
print("Missing Values in Sales Data After Handling:")
print(sales_data.isnull().sum())

print("\nMissing Values in Rentals Data After Handling:")
print(rentals_data.isnull().sum())






sales_constant_columns = find_constant_columns_with_details(sales_data, "Sales Data")
rentals_constant_columns = find_constant_columns_with_details(rentals_data, "rentals Data")
print(sales_constant_columns)
print(rentals_constant_columns)


# Check unique values and their counts in the original data
print("Unique values in 'property_usage_id' (sales_data_original):")
print(sales_data_original['property_usage_id'].value_counts(dropna=False))

# Check if the column is constant
if sales_data_original['property_usage_id'].nunique() == 1:
    print("\n'property_usage_id' is constant in the original dataset.")
else:
    print("\n'property_usage_id' is not constant in the original dataset.")






# Restore the original 'property_usage_id' column
sales_data['property_usage_id'] = sales_data_original['property_usage_id']

# Verify the unique values
print("Unique values in 'property_usage_id' after restoration:")
print(sales_data['property_usage_id'].value_counts())



# Impute missing values with the mode
sales_data['property_usage_id'] = sales_data['property_usage_id'].fillna(sales_data['property_usage_id'].mode()[0])


# Verify the missing values are handled
print("Missing values in 'property_usage_id':", sales_data['property_usage_id'].isnull().sum())



sales_data['property_usage_id'].value_counts()


sales_constant_columns = find_constant_columns_with_details(sales_data, "Sales Data")
rentals_constant_columns = find_constant_columns_with_details(rentals_data, "rentals Data")
print(sales_constant_columns)
print(rentals_constant_columns)





# Define the constant columns for each dataset
sales_constant_columns = ['transaction_datetime_year', 'req_from_year', 'req_from_day', 'req_to_year']
rentals_constant_columns = ['registration_date_year', 'req_from_year', 'req_from_day', 'req_to_year']

# Drop the constant columns from the datasets
sales_data.drop(columns=sales_constant_columns, inplace=True)
rentals_data.drop(columns=rentals_constant_columns, inplace=True)

# Verify the columns have been removed
print("Sales Data Columns After Dropping Constant Columns:")
print(sales_data.columns)

print("\nRentals Data Columns After Dropping Constant Columns:")
print(rentals_data.columns)






# Save the cleaned Sales Data to a CSV file
sales_data.to_csv("cleaned_sales_data.csv", index=False)
print("Cleaned Sales Data saved as 'cleaned_sales_data.csv'.")

# Save the cleaned Rentals Data to a CSV file
rentals_data.to_csv("cleaned_rentals_data.csv", index=False)
print("Cleaned Rentals Data saved as 'cleaned_rentals_data.csv'.")






print(sales_data.describe())
print(rentals_data.describe())



print(sales_data['property_type_en'].value_counts())
print(rentals_data['property_type_en'].value_counts())



import seaborn as sns
import matplotlib.pyplot as plt

# Correlation heatmap
sns.heatmap(sales_data.corr(), annot=True, fmt=".2f", cmap="coolwarm")
plt.show()






# Perform correlation analysis for numerical columns in both DataFrames

# Correlation analysis for sales_data
sales_corr = sales_data.corr(numeric_only=True)
print("Correlation Matrix for Sales Data:")
print(sales_corr)




# Correlation analysis for rentals_data
rentals_corr = rentals_data.corr(numeric_only=True)
print("\nCorrelation Matrix for Rentals Data:")
print(rentals_corr)



import matplotlib.pyplot as plt
import seaborn as sns

def plot_correlation_matrix_with_numbers(corr_matrix, title):
    """
    Plots a correlation matrix heatmap with numbers displayed in each cell.

    Parameters:
        corr_matrix (DataFrame): Correlation matrix to visualize.
        title (str): Title of the plot.
    """
    plt.figure(figsize=(12, 10))
    sns.heatmap(
        corr_matrix, 
        annot=True, 
        fmt=".2f", 
        cmap="coolwarm", 
        cbar=True, 
        square=True, 
        annot_kws={"size": 10}  # Adjust font size of annotations
    )
    plt.title(title, fontsize=16)
    plt.xticks(rotation=45, fontsize=10)
    plt.yticks(fontsize=10)
    plt.tight_layout()
    plt.show()

# Visualize correlation matrices
plot_correlation_matrix_with_numbers(sales_corr, "Sales Data Correlation Matrix")
plot_correlation_matrix_with_numbers(rentals_corr, "Rentals Data Correlation Matrix")
import matplotlib.pyplot as plt
import seaborn as sns

# Function to visualize correlation matrix
def plot_correlation_matrix(corr_matrix, title):
    plt.figure(figsize=(12, 10))
    sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", cbar=True)
    plt.title(title)
    plt.show()

# Plot correlation matrix for sales_data
plot_correlation_matrix(sales_corr, "Sales Data Correlation Matrix")

# Plot correlation matrix for rentals_data
plot_correlation_matrix(rentals_corr, "Rentals Data Correlation Matrix")













