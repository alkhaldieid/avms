import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


# Load the datasets
original_sales_data = pd.read_csv('../data/snp_dld_2024_transactions.csv', low_memory=False)
original_rentals_data = pd.read_csv('../data/snp_dld_2024_rents.csv', low_memory=False)


columns_to_drop_sales = [
    'transaction_number',  # ID column
    'entry_id',            # Metadata
    'meta_ts',             # Metadata timestamp
    'master_project_en',   # Very few non-null values
    'master_project_ar',   # Very few non-null values
    'property_type_ar',    # Duplicate of property_type_en
    'property_subtype_ar', # Duplicate of property_subtype_en
    'rooms_ar',            # Duplicate of rooms_en
    'project_name_ar',     # Duplicate of project_name_en
    'area_ar',             # Duplicate of area_en
    'nearest_landmark_ar', # Duplicate of nearest_landmark_en
    'nearest_metro_ar',    # Duplicate of nearest_metro_en
    'nearest_mall_ar',      # Duplicate of nearest_mall_en
    'parcel_id',  # not needed
    'transaction_type_en', #Duplicate of transaction_type_id
    'transaction_subtype_en', #Constant
    'transaction_subtype_id', #Constant
    'property_id', #Constant
    'property_type_id', #Constant
    'property_subtype_id', #Constant
    'building_age', #Constant
    'area_id', #Constant
    'is_freehold_text', #duplicate of is_freehold
    'property_usage_en', # Duplicate of property_usage_id
    'transaction_datetime_year',  #Constant after handling dates
    'req_from_year',  #Constant after handling dates
    'req_from_day',  #Constant after handling dates
    'req_to_year', #Constant after handling dates
    'parking' # Replaced with parking count
]

# List of columns to drop from rentals_data
columns_to_drop_rentals = [
    'ejari_contract_number', # ID column
    'land_property_id',      # Redundant ID column
    'entry_id',              # Metadata
    'meta_ts',               # Metadata timestamp
    'master_project_en',     # Very few non-null values
    'master_project_ar',     # Very few non-null values
    'property_type_ar',      # Duplicate of property_type_en
    'property_subtype_ar',   # Duplicate of property_subtype_en
    'property_usage_ar',     # Duplicate of property_usage_en
    'project_name_ar',       # Duplicate of project_name_en
    'area_ar',               # Duplicate of area_en
    'nearest_landmark_ar',   # Duplicate of nearest_landmark_en
    'nearest_metro_ar',      # Duplicate of nearest_metro_en
    'nearest_mall_ar',        # Duplicate of nearest_mall_en
    'property_id',   #constant
    'property_usage_id', #constant
    'area_id',   #constant
    'ejari_property_type_id',   #constant
    'ejari_property_sub_type_id',  #constant
    'parking', #97% missing data
    'parcel_id', # negative correlation
    'registration_date_year',  #Constant after handling dates
    'req_from_year',  #Constant after handling dates
    'req_from_day',  #Constant after handling dates
    'req_to_year',  #Constant after handling dates
    'req_from_year',  #Constant after handling dates
    'req_from_day', #Constant after handling dates
    'req_to_year',  #Constant after handling dates
    'registration_date_year',  #Constant after handling dates
    'req_from_year',  #Constant after handling dates
    'req_from_day',  #Constant after handling dates
    'req_to_year', #Constant after handling dates
    'is_freehold_text' # duplicate of is_freehold
]





def process_date_columns(df, date_columns):
    for col in date_columns:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')
            df[f"{col}_year"] = df[col].dt.year
            df[f"{col}_month"] = df[col].dt.month
            df[f"{col}_day"] = df[col].dt.day
            df[f"{col}_weekday"] = df[col].dt.weekday
            df[f"{col}_dayofyear"] = df[col].dt.dayofyear
    df.drop(columns=date_columns, inplace=True, errors='ignore')
    return df

sales_date_columns = ['transaction_datetime', 'req_from', 'req_to']
rentals_date_columns = ['registration_date', 'contract_start_date', 'contract_end_date', 'req_from', 'req_to']
sales_data = process_date_columns(original_sales_data, sales_date_columns)
rentals_data = process_date_columns(original_rentals_data, rentals_date_columns)





import re
import pandas as pd

def calculate_parking_count(value):
    """
    Calculate the parking count based on the given strategy:
    - Fields separated by commas are counted as separate spots unless 'to' is present.
    - If 'to' is present, calculate the range of parking spots.
    - Single identifiers like 'G-127' are counted as one spot.
    """
    if pd.isna(value):
        return 0  # No information

    total_count = 0
    fields = str(value).split(",")  # Split by commas
    for field in fields:
        field = field.strip()  # Remove extra whitespace
        if re.search(r"\bto\b", field, re.IGNORECASE):  # Check for 'to' (case insensitive)
            # Extract numbers in the range and calculate spots
            numbers = [int(num) for num in re.findall(r"\d+", field)]
            if len(numbers) == 2:  # Range is valid
                total_count += abs(numbers[1] - numbers[0]) + 1
        elif re.search(r"\d+", field):  # Single numeric identifier (e.g., G-127)
            total_count += 1
        else:
            total_count += 0  # Invalid or non-informative field
    return total_count

# Apply the logic to the `parking` column
sales_data['parking_count'] = sales_data['parking'].apply(calculate_parking_count)

# Save results for validation
sales_data[['parking', 'parking_count']].to_csv("processed_parking_count.csv", index=False)

# Example check
print(sales_data[['parking', 'parking_count']].tail())





sales_data.drop(columns=columns_to_drop_sales, inplace=True)
rentals_data.drop(columns=columns_to_drop_rentals, inplace=True)



sales_data.info()


rentals_data['version_number'].unique()


rentals_data['version_text'].unique()


sales_data.info()


def print_unique_values(df):
    """
    Iterate through all columns in a DataFrame.
    Skip columns of type float, and print unique values for other column types.

    Parameters:
        df (pd.DataFrame): The DataFrame to analyze.
    """
    for col in df.columns:
        if df[col].dtype == 'float64':
            continue  # Skip columns with float type
        print(f"Unique values in '{col}':")
        print(df[col].unique())
        print("\n")


print_unique_values(sales_data)


sales_data['parking'].describe()


print("Missing values in 'parking':", sales_data['parking'].isnull().sum())



sales_data['parking'].unique()


# Get the unique values of the 'parking' column
unique_parking_values = sales_data['parking'].unique()

# Convert unique values to a list (optional)
unique_parking_list = unique_parking_values.tolist()

# Save the unique values to a text file
with open("unique_parking_values.txt", "w") as file:
    for value in unique_parking_list:
        file.write(f"{value}\n")

print("Unique values saved to 'unique_parking_values.txt'")



import pandas as pd
import numpy as np
import re

def preprocess_parking_column(df, column_name):
    # Extract unique values
    unique_values = df[column_name].unique()
    
    # Categorize values
    def categorize_parking(value):
        if pd.isnull(value):
            return 'Unknown'
        elif re.match(r'^\d+$', str(value)):  # Pure numerical values
            return 'Count'
        elif re.match(r'[A-Z]-\d+', str(value)):  # Codes like B1-19
            return 'Parking Code'
        elif re.match(r'.*\d+.*', str(value)):  # Contains numbers in text
            return 'Descriptive with Numbers'
        else:  # Default for others
            return 'Other'
    
    # Apply categorization
    df[f'{column_name}_category'] = df[column_name].apply(categorize_parking)
    
    # Handle known categories separately if necessary
    df[f'{column_name}_processed'] = df[column_name].fillna('Unknown')
    
    # Example: Encode numerical counts as-is
    df[f'{column_name}_processed'] = df.apply(
        lambda x: x[column_name] if x[f'{column_name}_category'] == 'Count' else x[f'{column_name}_processed'],
        axis=1
    )
    
    return df

# Example usage
sales_data = preprocess_parking_column(sales_data, 'parking')

# Check the distribution
print(sales_data['parking_category'].value_counts())



sales_data['parking_category'].unique()


import re

# Extract parking counts from descriptions
def extract_parking_count(value):
    match = re.search(r'\b\d+\b', str(value))
    return int(match.group()) if match else None

# Categorize parking descriptions
def categorize_parking_description(value):
    if "limited" in str(value).lower():
        return "Limited"
    elif "available" in str(value).lower():
        return "Available"
    else:
        return "Other"

# Process parking feature
sales_data['parking_count'] = sales_data['parking'].apply(extract_parking_count)
sales_data['parking_category'] = sales_data['parking'].apply(categorize_parking_description)

# Handle 'Unknown'
sales_data['parking_count'] = sales_data['parking_count'].fillna(
    sales_data.groupby('area_en')['parking_count'].transform(
        lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 0)
    )
)


# Validate processed data
print(sales_data[['parking_count', 'parking_category']])



import re

# Function to extract parking count
def extract_parking_count(value):
    match = re.search(r'\b\d+\b', str(value))  # Find standalone numbers
    return int(match.group()) if match else 0  # Default to 0 if no match

# Function to categorize parking description
def categorize_parking(value):
    if "limited" in str(value).lower():
        return "Limited"
    elif "available" in str(value).lower():
        return "Available"
    elif re.search(r'[A-Za-z]*\d+[A-Za-z]*', str(value)):  # Matches codes like B1-19
        return "Code-Based"
    else:
        return "Unknown"

# Create new columns
sales_data['parking_available'] = sales_data['parking'].apply(
    lambda x: 1 if x != "Unknown" and pd.notnull(x) else 0
)
sales_data['parking_count'] = sales_data['parking'].apply(extract_parking_count)
sales_data['parking_category'] = sales_data['parking'].apply(categorize_parking)

# Validate the results
print(sales_data[['parking', 'parking_available', 'parking_count', 'parking_category']].head())






print(sales_data[['parking', 'parking_available', 'parking_count', 'parking_category']].tail())




import re
import pandas as pd

def calculate_parking_count(value):
    """
    Calculate the parking count based on the given strategy:
    - Fields separated by commas are counted as separate spots unless 'to' is present.
    - If 'to' is present, calculate the range of parking spots.
    - Single identifiers like 'G-127' are counted as one spot.
    """
    if pd.isna(value):
        return 0  # No information

    total_count = 0
    fields = str(value).split(",")  # Split by commas
    for field in fields:
        field = field.strip()  # Remove extra whitespace
        if re.search(r"\bto\b", field, re.IGNORECASE):  # Check for 'to' (case insensitive)
            # Extract numbers in the range and calculate spots
            numbers = [int(num) for num in re.findall(r"\d+", field)]
            if len(numbers) == 2:  # Range is valid
                total_count += abs(numbers[1] - numbers[0]) + 1
        elif re.search(r"\d+", field):  # Single numeric identifier (e.g., G-127)
            total_count += 1
        else:
            total_count += 0  # Invalid or non-informative field
    return total_count

# Apply the logic to the `parking` column
sales_data['parking_count'] = sales_data['parking'].apply(calculate_parking_count)

# Save results for validation
sales_data[['parking', 'parking_count']].to_csv("processed_parking_count.csv", index=False)

# Example check
print(sales_data[['parking', 'parking_count']].tail())



sales_data[["parking", "parking_category", "parking_count", "parking_available"]].tail()


sales_categorical_values = [
    'transaction_type_id',
    'registration_type_en',
    'property_usage_id',
    
]


sales_data['transaction_type_id'].unique()


original_sales_data['transaction_subtype_id'].unique()


sales_data['registration_type_en'].unique()


sales_data['property_usage_id'].unique()





# Display the number and percentage of missing values for Sales Data
print("Missing Values in Sales Data:")
missing_sales = sales_data.isnull().sum().to_frame(name="Missing Count")
missing_sales["Percentage"] = (missing_sales["Missing Count"] / len(sales_data)) * 100
print(missing_sales[missing_sales["Missing Count"] > 0])

# Display the number and percentage of missing values for Rentals Data
print("\nMissing Values in Rentals Data:")
missing_rentals = rentals_data.isnull().sum().to_frame(name="Missing Count")
missing_rentals["Percentage"] = (missing_rentals["Missing Count"] / len(rentals_data)) * 100
print(missing_rentals[missing_rentals["Missing Count"] > 0])



