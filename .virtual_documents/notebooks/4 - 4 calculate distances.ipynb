import pandas as pd
import numpy as np


area_coords = pd.read_csv('combined_geocoded_areas.csv')
landmarks = pd.read_csv('landmark_coordinates.csv')
malls = pd.read_csv('mall_coordinates.csv')
# Create a DataFrame with farthest and center points of Dubai with higher precision
dubai_extremes = pd.DataFrame({
    "point": ["north", "south", "east", "west", "center"],
    "latitude": [25.400000, 24.850000, 25.350000, 25.150000, 25.270000],
    "longitude": [55.270000, 55.270000, 55.600000, 55.000000, 55.300000]
})



# Load data
area_coords = pd.read_csv('combined_geocoded_areas.csv')
landmarks = pd.read_csv('landmark_coordinates.csv')
malls = pd.read_csv('mall_coordinates.csv')

# Dubai extremes DataFrame
dubai_extremes = pd.DataFrame({
    "point": ["north", "south", "east", "west", "center"],
    "latitude": [25.400000, 24.850000, 25.350000, 25.150000, 25.270000],
    "longitude": [55.270000, 55.270000, 55.600000, 55.000000, 55.300000]
})

# Haversine formula
def haversine(lat1, lon1, lat2, lon2):
    R = 6371  # Earth's radius in km
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2
    c = 2 * np.arcsin(np.sqrt(a))
    return R * c

# Create an empty list to store results
distances = []

# Iterate through areas
for _, area in area_coords.iterrows():
    area_name = area['area_en']
    area_lat = area['latitude']
    area_lon = area['longitude']
    
    # Calculate distances to landmarks
    for _, landmark in landmarks.iterrows():
        dist = haversine(area_lat, area_lon, landmark['latitude'], landmark['longitude'])
        distances.append({'area_en': area_name, 'type': 'landmark', 'name': landmark['landmark'], 'distance_km': dist})
    
    # Calculate distances to malls
    for _, mall in malls.iterrows():
        dist = haversine(area_lat, area_lon, mall['latitude'], mall['longitude'])
        distances.append({'area_en': area_name, 'type': 'mall', 'name': mall['mall'], 'distance_km': dist})
    
    # Calculate distances to Dubai extremes
    for _, extreme in dubai_extremes.iterrows():
        dist = haversine(area_lat, area_lon, extreme['latitude'], extreme['longitude'])
        distances.append({'area_en': area_name, 'type': 'extreme', 'name': extreme['point'], 'distance_km': dist})

# Convert results into a DataFrame
distance_df = pd.DataFrame(distances)

# Save the DataFrame to a CSV file
distance_df.to_csv('area_distances.csv', index=False)

# Display a sample of the result
print(distance_df)


distance_df.to_csv('distances.csv')


distance_df.drop(columns=['type'])


# Pivot the DataFrame
pivoted_df = distance_df.pivot(index='area_en', columns='name', values='distance_km')

# Flatten the columns and reset index
pivoted_df.columns.name = None  # Remove the name from the columns
pivoted_df.reset_index(inplace=True)

# Save to CSV (optional)
pivoted_df.to_csv("area_distance_matrix.csv", index=False)

# Display the cleaned DataFrame
print(pivoted_df)


sales_df = pd.read_csv('../data/snp_dld_2024_transactions.csv')
area_group = sales_df.groupby('area_en').agg({
    'project_name_en': 'count',        # Non-null project names
    'nearest_landmark_en': 'count',   # Non-null nearest landmarks
    "nearest_metro_en": 'count',      # Non-null nearest metro entries
    "nearest_mall_en": 'count',       # Non-null nearest mall entries
}).reset_index()

# Rename columns for clarity
area_group.rename(columns={
    'project_name_en': 'project_count',
    'nearest_landmark_en': 'landmark_count',
    'nearest_metro_en': 'metro_count',
    'nearest_mall_en': 'mall_count'
}, inplace=True)



area_group.to_csv('area_group_landmarks_count.csv')


area_group.info()


pivoted_df.info()


merged_df = pd.merge(area_group, pivoted_df, on='area_en', how='inner')
merged_df.info()


no_missing_sales = pd.read_csv('../data/sales_data_no_missing.csv')


no_missing_sales.info()


no_missing_merged_loc = pd.merge(no_missing_sales,merged_df, on="area_en", how="left")
no_missing_merged_loc.info()





no_missing_merged_loc['transaction_type_id'].unique()


# Convert transaction_type_id to a categorical type
no_missing_merged_loc['transaction_type_id'] = no_missing_merged_loc['transaction_type_id'].astype('category')

# Verify the change
print(no_missing_merged_loc['transaction_type_id'].dtypes)


no_missing_merged_loc['registration_type_en'].unique()


no_missing_merged_loc['registration_type_en'] = no_missing_merged_loc['registration_type_en'].astype('category')
no_missing_merged_loc['registration_type_encoded'] = no_missing_merged_loc['registration_type_en'].cat.codes
no_missing_merged_loc['registration_type_encoded'].unique()


print(no_missing_merged_loc['registration_type_en'].cat.categories)


no_missing_merged_loc = no_missing_merged_loc.drop(columns=['registration_type_en'])


no_missing_merged_loc['property_usage_id'].unique()





# Convert 'property_usage_id' to a categorical type with an explicit order
no_missing_merged_loc['property_usage_id'] = pd.Categorical(
    no_missing_merged_loc['property_usage_id'],
    categories=[1, 2],  # Explicitly specify the order
    ordered=True
)

# Verify the change
print(no_missing_merged_loc['property_usage_id'].dtype)


no_missing_merged_loc.info()





import numpy as np
import matplotlib.pyplot as plt

# Log-transform the amount to reduce skewness
log_amount = np.log1p(no_missing_merged_loc['amount'])  # Use log1p to handle zero values if any

# Plot the histogram of the log-transformed amount
plt.hist(log_amount, bins=50, edgecolor='k', alpha=0.7)
plt.title("Log-Transformed Distribution of Amount")
plt.xlabel("Log(Amount)")
plt.ylabel("Frequency")
plt.show()



# Min-Max Normalization
amount_min = no_missing_merged_loc['amount'].min()
amount_max = no_missing_merged_loc['amount'].max()

no_missing_merged_loc['amount_normalized'] = (no_missing_merged_loc['amount'] - amount_min) / (amount_max - amount_min)

# Z-Score Standardization
amount_mean = no_missing_merged_loc['amount'].mean()
amount_std = no_missing_merged_loc['amount'].std()

no_missing_merged_loc['amount_standardized'] = (no_missing_merged_loc['amount'] - amount_mean) / amount_std

# Combine Normalization and Standardization
normalized_amount = (no_missing_merged_loc['amount'] - amount_min) / (amount_max - amount_min)
normalized_mean = normalized_amount.mean()
normalized_std = normalized_amount.std()

no_missing_merged_loc['amount_scaled'] = (normalized_amount - normalized_mean) / normalized_std

# Display results
print(no_missing_merged_loc[['amount', 'amount_normalized', 'amount_standardized', 'amount_scaled']])




# Improved Visualization of Distributions
plt.figure(figsize=(15, 6))

plt.subplot(1, 3, 1)
plt.hist(no_missing_merged_loc['amount'], bins=50, color='blue', alpha=0.7)
plt.yscale('log')  # Log scale for better visualization of skewed data
plt.title('Original Amount Distribution (Log Scale)')
plt.xlabel('Amount')
plt.ylabel('Frequency')

plt.subplot(1, 3, 2)
plt.hist(no_missing_merged_loc['amount_normalized'], bins=50, color='green', alpha=0.7)
plt.title('Normalized Amount Distribution')
plt.xlabel('Normalized Amount')
plt.ylabel('Frequency')

plt.subplot(1, 3, 3)
plt.hist(no_missing_merged_loc['amount_scaled'], bins=50, color='orange', alpha=0.7)
plt.title('Scaled Amount Distribution')
plt.xlabel('Scaled Amount')
plt.ylabel('Frequency')

plt.tight_layout()
plt.savefig('improved_tmpfig.png')
plt.show()




print(amount_min)
print(amount_max)
print(amount_mean)
print(amount_std)


from sklearn.preprocessing import QuantileTransformer

quantile_transformer = QuantileTransformer(output_distribution='normal')
no_missing_merged_loc['amount_quantile_scaled'] = quantile_transformer.fit_transform(
    no_missing_merged_loc[['amount']]
)


import matplotlib.pyplot as plt
import seaborn as sns

# Create a figure with subplots
plt.figure(figsize=(12, 8))

# Original Amount Distribution
plt.subplot(2, 2, 1)
sns.histplot(no_missing_merged_loc['amount'], bins=50, kde=True, color='blue', alpha=0.6)
plt.title('Original Amount Distribution')
plt.xlabel('Amount')
plt.ylabel('Frequency')
plt.yscale('log')  # Log scale for better visibility of distribution

# Min-Max Normalized Amount Distribution
plt.subplot(2, 2, 2)
sns.histplot(no_missing_merged_loc['amount_normalized'], bins=50, kde=True, color='green', alpha=0.6)
plt.title('Min-Max Normalized Amount Distribution')
plt.xlabel('Normalized Amount')
plt.ylabel('Frequency')

# Quantile Transformed (Normal Distribution) Amount
plt.subplot(2, 2, 3)
sns.histplot(no_missing_merged_loc['amount_quantile_scaled'], bins=50, kde=True, color='orange', alpha=0.6)
plt.title('Quantile Transformed Amount Distribution')
plt.xlabel('Quantile Transformed Amount')
plt.ylabel('Frequency')

# Combined KDE Plot for Comparison
plt.subplot(2, 2, 4)
sns.kdeplot(no_missing_merged_loc['amount'], color='blue', label='Original', bw_adjust=0.5)
sns.kdeplot(no_missing_merged_loc['amount_normalized'], color='green', label='Min-Max Normalized', bw_adjust=0.5)
sns.kdeplot(no_missing_merged_loc['amount_quantile_scaled'], color='orange', label='Quantile Transformed', bw_adjust=0.5)
plt.title('Kernel Density Estimate Comparison')
plt.xlabel('Value')
plt.ylabel('Density')
plt.legend()

plt.tight_layout()
plt.show()






no_missing_merged_loc.info()


# Drop the specified columns
columns_to_drop = ['amount_scaled', 'amount_normalized', 'amount_standardized', 'amount_quantile_scaled']
no_missing_merged_loc = no_missing_merged_loc.drop(columns=columns_to_drop)

# Verify the columns are dropped
print(no_missing_merged_loc.info())



no_missing_merged_loc['total_buyer'].unique()


no_missing_merged_loc['total_seller'].unique()


# Adding an interaction feature
no_missing_merged_loc['buyer_to_seller_ratio'] = no_missing_merged_loc['total_buyer'] / (
    no_missing_merged_loc['total_seller'] + 1
)



no_missing_merged_loc['buyer_to_seller_ratio']


import matplotlib.pyplot as plt

# Plot distributions
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.hist(no_missing_merged_loc['transaction_size_sqm'], bins=50, color='blue', alpha=0.7)
plt.title("Transaction Size Distribution")
plt.xlabel("Transaction Size (sqm)")
plt.ylabel("Frequency")

plt.subplot(1, 2, 2)
plt.hist(no_missing_merged_loc['property_size_sqm'], bins=50, color='green', alpha=0.7)
plt.title("Property Size Distribution")
plt.xlabel("Property Size (sqm)")
plt.ylabel("Frequency")

plt.tight_layout()
plt.show()






no_missing_merged_loc['is_offplan']


no_missing_merged_loc['is_freehold']


no_missing_merged_loc['is_freehold_encoded']


no_missing_merged_loc['property_type_en'].unique()


categorical_columns = [
    'is_offplan', 'is_freehold', 'property_type_en',
    'property_subtype_en', 'project_name_en', 
]


for col in categorical_columns:
    print(no_missing_merged_loc[col].unique())


no_missing_merged_loc['property_subtype_en'].unique()


# Define grouping for all unique subtypes
grouped_subtypes = {
    'Residential': [
        'Flat', 'Villa', 'Residential', 'Residential Flats', 
        'Residential / Villas', 'Residential / Attached Villas', 'Stacked Townhouses'
    ],
    'Commercial': [
        'Shop', 'Office', 'Commercial', 'Commercial / Offices / Residential', 
        'Show Rooms', 'Building', 'Unit', 'General Use'
    ],
    'Institutional': [
        'School', 'Hospital', 'Clinic', 'Health Club', 'Gymnasium', 'Exhbition Center', 'Consulate'
    ],
    'Industrial': [
        'Industrial', 'Warehouse', 'Workshop', 'Sized Partition', 'Electricity Station', 'Labor Camp'
    ],
    'Hospitality': [
        'Hotel', 'Hotel Apartment', 'Hotel Rooms', 'Sports Club'
    ],
    'Infrastructure': [
        'Airport', 'Petrol Station'
    ],
    'Land': [
        'Land', 'Agricultural'
    ],
    'Other': [
        'Government Housing'
    ]
}

# Ensure all unique values are covered
all_defined_values = [item for sublist in grouped_subtypes.values() for item in sublist]
undefined_values = set(no_missing_merged_loc['property_subtype_en'].unique()) - set(all_defined_values)

# If there are undefined values, add them to the "Other" category
if undefined_values:
    print(f"Undefined values: {undefined_values}")
    grouped_subtypes['Other'].extend(undefined_values)

# Map subtypes to broader categories
no_missing_merged_loc['property_subtype_grouped'] = no_missing_merged_loc['property_subtype_en'].map(
    lambda x: next((k for k, v in grouped_subtypes.items() if x in v), 'Other')
)





no_missing_merged_loc['property_subtype_grouped'].unique()


# Encode the grouped subtypes
from sklearn.preprocessing import LabelEncoder
grouped_le = LabelEncoder()
no_missing_merged_loc['property_subtype_encoded'] = grouped_le.fit_transform(no_missing_merged_loc['property_subtype_grouped'])

# Drop the original column
no_missing_merged_loc.drop(columns=['property_subtype_en', 'property_subtype_grouped'], inplace=True)

# Display the mapping for grouped subtypes
grouped_mapping = dict(zip(grouped_le.classes_, grouped_le.transform(grouped_le.classes_)))
print("Grouped Subtypes Mapping:")
print(grouped_mapping)


no_missing_merged_loc.info()





no_missing_merged_loc = no_missing_merged_loc.drop(columns=['is_freehold'])
no_missing_merged_loc.info()


# Drop the specified columns
columns_to_drop = [
    'project_name_en', 
    'area_en', 
    'nearest_landmark_en', 
    'nearest_metro_en', 
    'nearest_mall_en'
]

no_missing_merged_loc.drop(columns=columns_to_drop, inplace=True)

# Verify the changes
print(no_missing_merged_loc.info())



no_missing_merged_loc['property_type_en'].unique()


from sklearn.preprocessing import LabelEncoder

# Convert 'is_offplan' to categorical and encode
is_offplan_encoder = LabelEncoder()
no_missing_merged_loc['is_offplan_encoded'] = is_offplan_encoder.fit_transform(
    no_missing_merged_loc['is_offplan']
)
print("is_offplan mapping:", dict(zip(is_offplan_encoder.classes_, is_offplan_encoder.transform(is_offplan_encoder.classes_))))

# Convert 'property_type_en' to categorical and encode
property_type_encoder = LabelEncoder()
no_missing_merged_loc['property_type_encoded'] = property_type_encoder.fit_transform(
    no_missing_merged_loc['property_type_en']
)
print("property_type_en mapping:", dict(zip(property_type_encoder.classes_, property_type_encoder.transform(property_type_encoder.classes_))))

# Drop the original columns
no_missing_merged_loc.drop(columns=['is_offplan', 'property_type_en'], inplace=True)

# Verify the changes
print(no_missing_merged_loc.info())



no_missing_merged_loc.to_csv('clean_for_training.csv')



