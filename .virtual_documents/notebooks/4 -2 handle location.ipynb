import pandas as pd


# Load the datasets
original_sales_data = pd.read_csv('../data/snp_dld_2024_transactions.csv', low_memory=False)
original_rentals_data = pd.read_csv('../data/snp_dld_2024_rents.csv', low_memory=False)
# Load the processed dfs
sales_data = pd.read_csv('../data/sales_data_no_missing.csv', low_memory=False)
rentals_data = pd.read_csv('../data/rentals_data_no_missing.csv', low_memory=False)


original_sales_data.info()


location_columns_sales = [
    "project_name_en",
    "project_name_ar",
    "area_en",
    "area_ar",
    "area_id",
    "nearest_landmark_en",
    "nearest_landmark_ar",
    "nearest_metro_en",
    "nearest_metro_ar",
    "nearest_mall_en",
    "nearest_mall_ar",
    "master_project_en",
    "master_project_ar"
]



location_data_sales = original_sales_data[location_columns_sales]


aarea_group = location_data_sales.groupby('area_en').agg({
    'project_name_en': 'count',        # Non-null project names
    'nearest_landmark_en': 'count',   # Non-null nearest landmarks
    "nearest_metro_en": 'count',      # Non-null nearest metro entries
    "nearest_mall_en": 'count',       # Non-null nearest mall entries
}).reset_index()

# Rename columns for clarity
area_group.rename(columns={
    'project_name_en': 'project_count',
    'nearest_landmark_en': 'landmark_count',
    'nearest_metro_en': 'metro_count',
    'nearest_mall_en': 'mall_count'
}, inplace=True)




area_group.tail()


from geopy.geocoders import Nominatim

# Initialize the geolocator
geolocator = Nominatim(user_agent="geoapi")

# Arabic address or place name
location = geolocator.geocode("دبي")


location


df = location_data_sales


import pandas as pd
from geopy.geocoders import Nominatim
from geopy.distance import geodesic
import time
import logging
import os
from tqdm import tqdm  # For progress bar

# Initialize logging
logging.basicConfig(filename="geocoding_errors.log", level=logging.ERROR)

# Initialize the geolocator with a higher timeout
geolocator = Nominatim(user_agent="geoapi", timeout=10)

# Define Dubai's approximate coordinates
dubai_coordinates = (25.276987, 55.296249)

# Cache to avoid redundant requests
coordinate_cache = {}

# Function to check if coordinates are within Dubai
def is_within_dubai(lat, lon, dubai_center=dubai_coordinates, max_distance_km=50):
    """Check if a location is within a reasonable distance of Dubai."""
    if lat is None or lon is None:
        return False
    distance = geodesic((lat, lon), dubai_center).km
    return distance <= max_distance_km

# Function to get coordinates with caching, retries, and rate limiting
def get_coordinates(location):
    """Fetch coordinates for a given location."""
    if location in coordinate_cache:
        return coordinate_cache[location]  # Return from cache if available

    try:
        time.sleep(1)  # Respect rate limit (1 request per second)
        loc = geolocator.geocode(location)
        if loc:
            lat, lon = loc.latitude, loc.longitude
            coordinate_cache[location] = (lat, lon)  # Save to cache
            return lat, lon
    except Exception as e:
        logging.error(f"Error fetching coordinates for {location}: {e}")
    return None, None

# Function to fetch coordinates for an area with fallback logic
def fetch_coordinates_for_area(row):
    """Fetch coordinates for an area using multiple fallback fields."""
    fields = [
        'area_en', 'area_ar', 'project_name_en', 'project_name_ar',
        'nearest_landmark_en', 'nearest_landmark_ar', 
        'nearest_metro_en', 'nearest_metro_ar', 
        'nearest_mall_en', 'nearest_mall_ar', 
        'master_project_en', 'master_project_ar'
    ]
    
    for field in fields:
        location = row.get(field)
        if pd.notnull(location):  # Only process non-null locations
            lat, lon = get_coordinates(location)
            if is_within_dubai(lat, lon):  # Validate if coordinates are in Dubai
                return lat, lon

    # Return None if no valid coordinates found
    return None, None

# Check if a saved CSV exists for coordinates
coordinates_csv = "area_coordinates.csv"
if os.path.exists(coordinates_csv):
    # Load cached coordinates from CSV
    print(f"Loading cached coordinates from {coordinates_csv}...")
    unique_areas = pd.read_csv(coordinates_csv)
    coordinate_cache = {
        row['area_en']: (row['latitude'], row['longitude'])
        for _, row in unique_areas.iterrows()
    }
else:
    # Preprocess to get unique areas
    unique_areas = df.groupby('area_en').first().reset_index()

    # Apply geocoding to unique areas with a progress bar
    tqdm.pandas(desc="Geocoding Areas")  # Add description to progress bar
    unique_areas[['latitude', 'longitude']] = unique_areas.progress_apply(
        fetch_coordinates_for_area, axis=1, result_type='expand'
    )

    # Save unique area coordinates to CSV for future runs
    unique_areas.to_csv(coordinates_csv, index=False)
    print(f"Saved coordinates to {coordinates_csv}.")

# Merge the coordinates back to the original dataset
df = pd.merge(df, unique_areas[['area_en', 'latitude', 'longitude']], on='area_en', how='left')

# Save the full dataset with coordinates to a CSV
final_csv = "dataset_with_coordinates.csv"
df.to_csv(final_csv, index=False)
print(f"Saved the dataset with coordinates to {final_csv}.")




coordinate_cache


# Check if each area_en has coordinates
unique_areas['has_coordinates'] = unique_areas[['latitude', 'longitude']].notnull().all(axis=1)

# Print areas without coordinates
missing_coordinates = unique_areas[~unique_areas['has_coordinates']]
if not missing_coordinates.empty:
    print(f"Areas without coordinates:\n{missing_coordinates[['area_en']]}")
else:
    print("All areas have coordinates!")


missing_coordinates[['area_en']].isnull()


unique_area = location_data_sales['area_en'].unique()


unique_areas


import pandas as pd
import os
from geopy.geocoders import Nominatim
from concurrent.futures import ThreadPoolExecutor
import time

# Initialize geopy geolocator
geolocator = Nominatim(user_agent="geoapi")
geocoded_locations = {}  # Cache for geocoded locations

# Define Dubai's geographic coordinate range (expanded for tolerance)
DUBAI_LAT_RANGE = (24.0, 26.5)
DUBAI_LON_RANGE = (54.0, 56.0)

def is_within_dubai(lat, lon):
    """Check if coordinates are within Dubai."""
    return DUBAI_LAT_RANGE[0] <= lat <= DUBAI_LAT_RANGE[1] and DUBAI_LON_RANGE[0] <= lon <= DUBAI_LON_RANGE[1]

def fetch_lat_long(location_name):
    """Fetch latitude and longitude for a location."""
    if location_name in geocoded_locations:
        return geocoded_locations[location_name]
    try:
        location = geolocator.geocode(location_name)
        if location:
            geocoded_locations[location_name] = (location.latitude, location.longitude)
            return location.latitude, location.longitude
    except Exception as e:
        pass
    geocoded_locations[location_name] = (None, None)
    return None, None

def process_area(area_name):
    """Fetch coordinates for an area."""
    if area_name == "Unknown" or pd.isna(area_name):
        return area_name, None, None

    lat, lon = fetch_lat_long(area_name)
    if lat is not None and lon is not None and is_within_dubai(lat, lon):
        print(f"Found valid coordinates for {area_name}: ({lat}, {lon})")
        return area_name, lat, lon

    print(f"No valid coordinates found for {area_name}")
    return area_name, None, None

def geocode_areas(dataframe):
    """Geocode all areas using multithreading."""
    areas = dataframe['area_en'].unique()
    print(f"Number of unique areas to geocode: {len(areas)}")

    with ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(process_area, areas))

    geocoded_df = pd.DataFrame(results, columns=['area_en', 'latitude', 'longitude'])
    return geocoded_df

# Main Logic
geocoded_file = "geocoded_area_new.csv"

if os.path.exists(geocoded_file):
    print(f"Loading existing geocoded file: {geocoded_file}")
    geocoded_df = pd.read_csv(geocoded_file)
else:
    # Assuming sales_data is your main dataset; update this if different
    sales_data = pd.read_csv("../data/snp_dld_2024_transactions.csv")  # Replace with the actual path to your dataset
    geocoded_df = geocode_areas(sales_data)

    # Save the geocoded areas
    geocoded_df.to_csv(geocoded_file, index=False)
    print(f"Geocoded data saved to {geocoded_file}")

# Remaining missing coordinates
remaining_missing = geocoded_df[geocoded_df['latitude'].isnull() & geocoded_df['longitude'].isnull()]
print(f"\nNumber of areas with missing coordinates: {len(remaining_missing)}")



import pandas as pd
from geopy.geocoders import Nominatim
import time

# Initialize geopy geolocator
geolocator = Nominatim(user_agent="geoapi", timeout=10)

# Define Dubai's geographic coordinate range
DUBAI_LAT_RANGE = (24.0, 26.5)
DUBAI_LON_RANGE = (54.0, 56.0)

def is_within_dubai(lat, lon):
    """Check if coordinates are within Dubai."""
    return DUBAI_LAT_RANGE[0] <= lat <= DUBAI_LAT_RANGE[1] and DUBAI_LON_RANGE[0] <= lon <= DUBAI_LON_RANGE[1]

def fetch_lat_long(location_name):
    """Fetch latitude and longitude for a location."""
    try:
        location = geolocator.geocode(location_name)
        if location:
            return location.latitude, location.longitude
    except Exception as e:
        print(f"Error fetching {location_name}: {e}")
    return None, None

def build_area_dict(dataframe):
    """Build a dictionary with area_en as keys and other location-related attributes as values."""
    grouped = dataframe.groupby('area_en').agg({
        'area_ar': lambda x: list(x.unique()),
        'project_name_en': lambda x: list(x.unique()),
        'project_name_ar': lambda x: list(x.unique()),
        'nearest_landmark_en': lambda x: list(x.unique()),
        'nearest_landmark_ar': lambda x: list(x.unique()),
        'nearest_metro_en': lambda x: list(x.unique()),
        'nearest_metro_ar': lambda x: list(x.unique()),
        'nearest_mall_en': lambda x: list(x.unique()),
        'nearest_mall_ar': lambda x: list(x.unique()),
        'master_project_en': lambda x: list(x.unique()),
        'master_project_ar': lambda x: list(x.unique()),
    }).reset_index()
    
    area_dict = {
        row['area_en']: (
            row['area_ar'], 
            row['project_name_en'], row['project_name_ar'], 
            row['nearest_landmark_en'], row['nearest_landmark_ar'], 
            row['nearest_metro_en'], row['nearest_metro_ar'], 
            row['nearest_mall_en'], row['nearest_mall_ar'], 
            row['master_project_en'], row['master_project_ar']
        )
        for _, row in grouped.iterrows()
    }
    return area_dict

def infer_coordinates(area_dict):
    """Infer coordinates for each area_en entry."""
    geocoded_results = {}
    
    for area_en, attributes in area_dict.items():
        if area_en == "Unknown":
            continue  # Skip 'Unknown' entries
        
        print(f"Processing {area_en}...")
        # Step 1: Try area_en
        lat, lon = fetch_lat_long(area_en)
        if lat and lon and is_within_dubai(lat, lon):
            geocoded_results[area_en] = (lat, lon)
            print(f"Found valid coordinates for {area_en} using area_en: ({lat}, {lon})")
            continue
        
        # Step 2: Try area_ar
        for area_ar in attributes[0]:
            lat, lon = fetch_lat_long(area_ar)
            if lat and lon and is_within_dubai(lat, lon):
                geocoded_results[area_en] = (lat, lon)
                print(f"Found valid coordinates for {area_en} using area_ar: ({lat}, {lon})")
                break
        if area_en in geocoded_results:
            continue

        # Step 3: Try project_name_en
        for project in attributes[1]:
            lat, lon = fetch_lat_long(project)
            if lat and lon and is_within_dubai(lat, lon):
                geocoded_results[area_en] = (lat, lon)
                print(f"Found valid coordinates for {area_en} using project_name_en: ({lat}, {lon})")
                break
        if area_en in geocoded_results:
            continue

        # Step 4: Try project_name_ar
        for project in attributes[2]:
            lat, lon = fetch_lat_long(project)
            if lat and lon and is_within_dubai(lat, lon):
                geocoded_results[area_en] = (lat, lon)
                print(f"Found valid coordinates for {area_en} using project_name_ar: ({lat}, {lon})")
                break
        if area_en in geocoded_results:
            continue

        # Step 5: Try nearest_landmark_en
        for landmark in attributes[3]:
            lat, lon = fetch_lat_long(landmark)
            if lat and lon and is_within_dubai(lat, lon):
                geocoded_results[area_en] = (lat, lon)
                print(f"Found valid coordinates for {area_en} using nearest_landmark_en: ({lat}, {lon})")
                break
        if area_en in geocoded_results:
            continue

        # Step 6: Try nearest_landmark_ar
        for landmark in attributes[4]:
            lat, lon = fetch_lat_long(landmark)
            if lat and lon and is_within_dubai(lat, lon):
                geocoded_results[area_en] = (lat, lon)
                print(f"Found valid coordinates for {area_en} using nearest_landmark_ar: ({lat}, {lon})")
                break
        if area_en in geocoded_results:
            continue

        # Step 7: Try other location fields (nearest_metro, mall, etc.)
        for field in attributes[5:]:
            for location in field:
                lat, lon = fetch_lat_long(location)
                if lat and lon and is_within_dubai(lat, lon):
                    geocoded_results[area_en] = (lat, lon)
                    print(f"Found valid coordinates for {area_en} using {location}: ({lat}, {lon})")
                    break
            if area_en in geocoded_results:
                break
        
        if area_en not in geocoded_results:
            print(f"Could not find valid coordinates for {area_en}")
        
        # Respect API rate limits
        time.sleep(1)

    return geocoded_results

# Main logic
# Load your dataset
sales_data = pd.read_csv("../data/snp_dld_2024_transactions.csv")  # Replace with your dataset's path
area_dict = build_area_dict(sales_data)
geocoded_results = infer_coordinates(area_dict)

# Save the results
geocoded_df = pd.DataFrame.from_dict(geocoded_results, orient='index', columns=['latitude', 'longitude'])
geocoded_df.reset_index(inplace=True)
geocoded_df.rename(columns={'index': 'area_en'}, inplace=True)
geocoded_df.to_csv("geocoded_area_new.csv", index=False)
print("Geocoding completed and saved to geocoded_area_new.csv")



geocoded_df.info()


# Assuming 'sales_data' is the original dataset with all area_en values
# Get all unique area_en values from the original dataset
all_area_en = sales_data['area_en'].unique()

# Get area_en values present in geocoded_df
geocoded_areas = geocoded_df['area_en'].unique()

# Find missing area_en
missing_areas = set(all_area_en) - set(geocoded_areas)

# Print missing area_en names
print("Areas with missing coordinates:", missing_areas)

# Optionally convert to a list if needed
missing_area_list = list(missing_areas)



len(missing_area_list)


geocoded_df.head()


# List of missing areas
missing_area_list = [
    'Al Hebiah Second', 'Al Warsan First', 'LIWAN 2', 'Al Warsan Third', 
    'Dubai Investment Park First', 'Al Hebiah Fourth', 'Hessyan Second', 
    'THE FIELD', 'Warsan Fourth', 'Al Hebiah Sixth', 'AL Athbah', 
    'Al Thanyah Fifth', 'AL WAHA', 'Al Aweer Second', 'Lehbab First', 
    'Hessyan First', 'LIWAN', 'THE VALLEY', 'Lehbab Second', 
    "Me'Aisem Second", 'Al Hebiah Third', 'Al Aweer First', 
    'Madinat Hind 3', 'Al Rowaiyah Third', 'Al Hebiah Fifth', 'Muragab'
]

# Create a DataFrame structure for manual entry
missing_coords_df = pd.DataFrame({
    'area_en': missing_area_list,
    'latitude': [None] * len(missing_area_list),  # Placeholder for latitude
    'longitude': [None] * len(missing_area_list)  # Placeholder for longitude
})

# Save the DataFrame to a CSV file for manual entry if needed
missing_coords_df.to_csv('missing_coords_manual.csv', index=False)

# Display the structure
print(missing_coords_df.head())


manual_coords_df = pd.read_csv('missing_coords_manual.csv')
manual_coords_df.info()


# Merge the datasets
combined_geocoded_df = pd.concat([geocoded_df, manual_coords_df], ignore_index=True)

# Save the combined DataFrame to a new CSV file
combined_geocoded_df.to_csv("combined_geocoded_areas.csv", index=False)


combined_geocoded_df.info()



