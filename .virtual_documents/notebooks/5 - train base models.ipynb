import pandas as pd


no_missing_merged_loc = pd.read_csv('clean_for_training.csv')


no_missing_merged_loc.drop(columns='Unnamed: 0', inplace=True)


from sklearn.preprocessing import QuantileTransformer
from sklearn.model_selection import train_test_split
import pickle

# Define features (X) and target (y)
X = no_missing_merged_loc.drop(columns=['amount'])  # Replace 'amount' with your target column if different
y = no_missing_merged_loc['amount']

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Further split training data for meta-learner (optional)
X_train_base, X_val_meta, y_train_base, y_val_meta = train_test_split(X_train, y_train, test_size=0.25, random_state=42)

# Initialize QuantileTransformers
qt_amount = QuantileTransformer(output_distribution='normal', random_state=42)
qt_size = QuantileTransformer(output_distribution='normal', random_state=42)

# Fit transformers on training data
# Fit qt_size on transaction and property sizes
qt_size.fit(X_train[['transaction_size_sqm', 'property_size_sqm']])

# Fit qt_amount on the entire `y_train` dataset
qt_amount.fit(y_train.values.reshape(-1, 1))

# Transform sizes in the training, validation, and test sets
X_train_base[['transaction_size_sqm', 'property_size_sqm']] = qt_size.transform(
    X_train_base[['transaction_size_sqm', 'property_size_sqm']]
)
X_val_meta[['transaction_size_sqm', 'property_size_sqm']] = qt_size.transform(
    X_val_meta[['transaction_size_sqm', 'property_size_sqm']]
)
X_test[['transaction_size_sqm', 'property_size_sqm']] = qt_size.transform(
    X_test[['transaction_size_sqm', 'property_size_sqm']]
)

# Transform target variable in the training, validation, and test sets
y_train_base = qt_amount.transform(y_train_base.values.reshape(-1, 1)).flatten()
y_val_meta = qt_amount.transform(y_val_meta.values.reshape(-1, 1)).flatten()
y_test = qt_amount.transform(y_test.values.reshape(-1, 1)).flatten()

# Save the transformers for later use
with open("qt_amount.pkl", "wb") as f:
    pickle.dump(qt_amount, f)
with open("qt_size.pkl", "wb") as f:
    pickle.dump(qt_size, f)

# Print dataset sizes
print(f"Training set (Base models): {X_train_base.shape}")
print(f"Validation set (Meta-learner): {X_val_meta.shape}")
print(f"Test set: {X_test.shape}")





y_train_base.mean()


import numpy as np

# Check the transformed training data
print("Transformed y_train_base (mean, std):", np.mean(y_train_base), np.std(y_train_base))
print("Transformed transaction_size_sqm (mean, std):",
      np.mean(X_train_base['transaction_size_sqm']), np.std(X_train_base['transaction_size_sqm']))
print("Transformed property_size_sqm (mean, std):",
      np.mean(X_train_base['property_size_sqm']), np.std(X_train_base['property_size_sqm']))

# Verify that the transformed data is approximately normal
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))

plt.subplot(1, 3, 1)
plt.hist(y_train_base, bins=30, color='blue', alpha=0.7)
plt.title("Transformed Amount (y_train_base) Distribution")
plt.xlabel("Scaled Amount")
plt.ylabel("Frequency")

plt.subplot(1, 3, 2)
plt.hist(X_train_base['transaction_size_sqm'], bins=30, color='green', alpha=0.7)
plt.title("Transformed Transaction Size Distribution")
plt.xlabel("Scaled Transaction Size")
plt.ylabel("Frequency")

plt.subplot(1, 3, 3)
plt.hist(X_train_base['property_size_sqm'], bins=30, color='orange', alpha=0.7)
plt.title("Transformed Property Size Distribution")
plt.xlabel("Scaled Property Size")
plt.ylabel("Frequency")

plt.tight_layout()
plt.show()



import optuna
from sklearn.model_selection import cross_val_score
from base_models import optimize_xgboost, optimize_random_forest, optimize_svr

# General optimization function
def optimize_model(optimize_func, X_train, y_train, n_trials=20, cv=5):
    """
    Optimize a model using Optuna and cross-validation.

    Args:
        optimize_func: Function defining the model and parameters for optimization.
        X_train: Training features.
        y_train: Training target.
        n_trials: Number of optimization trials.
        cv: Number of cross-validation folds.

    Returns:
        Best parameters and corresponding model performance.
    """
    def objective(trial):
        model = optimize_func(trial, X_train, y_train)
        cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2')
        mean_cv_score = cv_scores.mean()
        print(f"Trial {trial.number} | CV Score: {mean_cv_score:.4f}")
        return mean_cv_score

    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=n_trials)
    print(f"Best Score: {study.best_value:.4f}")
    return study.best_params, study.best_value

# Optimize XGBoost
print("\n--- Optimizing XGBoost ---")
best_params_xgb, best_score_xgb = optimize_model(optimize_xgboost, X_train_base, y_train_base, n_trials=20)
print("Best parameters for XGBoost:", best_params_xgb)

# Optimize Random Forest
print("\n--- Optimizing Random Forest ---")
best_params_rf, best_score_rf = optimize_model(optimize_random_forest, X_train_base, y_train_base, n_trials=20)
print("Best parameters for Random Forest:", best_params_rf)

# Optimize SVR
print("\n--- Optimizing SVR ---")
best_params_svr, best_score_svr = optimize_model(optimize_svr, X_train_base, y_train_base, n_trials=20)
print("Best parameters for SVR:", best_params_svr)




