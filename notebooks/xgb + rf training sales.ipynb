{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf8b02e-1c3c-4f2c-81b4-3b87991baf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d5b439c-98aa-4989-8f17-4d9bb5eae4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (Base models): (97683, 51)\n",
      "Validation set (Meta-learner): (32561, 51)\n",
      "Test set: (32562, 51)\n"
     ]
    }
   ],
   "source": [
    "no_missing_merged_loc = pd.read_csv('clean_for_training.csv')\n",
    "no_missing_merged_loc.drop(columns='Unnamed: 0', inplace=True)\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = no_missing_merged_loc.drop(columns=['amount'])  # Replace 'amount' with your target column if different\n",
    "y = no_missing_merged_loc['amount']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split training data for meta-learner (optional)\n",
    "X_train_base, X_val_meta, y_train_base, y_val_meta = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize QuantileTransformers\n",
    "qt_amount = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "qt_size = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "\n",
    "# Fit transformers on training data\n",
    "# Fit qt_size on transaction and property sizes\n",
    "qt_size.fit(X_train[['transaction_size_sqm', 'property_size_sqm']])\n",
    "\n",
    "# Fit qt_amount on the entire `y_train` dataset\n",
    "qt_amount.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Transform sizes in the training, validation, and test sets\n",
    "X_train_base[['transaction_size_sqm', 'property_size_sqm']] = qt_size.transform(\n",
    "    X_train_base[['transaction_size_sqm', 'property_size_sqm']]\n",
    ")\n",
    "X_val_meta[['transaction_size_sqm', 'property_size_sqm']] = qt_size.transform(\n",
    "    X_val_meta[['transaction_size_sqm', 'property_size_sqm']]\n",
    ")\n",
    "X_test[['transaction_size_sqm', 'property_size_sqm']] = qt_size.transform(\n",
    "    X_test[['transaction_size_sqm', 'property_size_sqm']]\n",
    ")\n",
    "\n",
    "# Transform target variable in the training, validation, and test sets\n",
    "y_train_base = qt_amount.transform(y_train_base.values.reshape(-1, 1)).flatten()\n",
    "y_val_meta = qt_amount.transform(y_val_meta.values.reshape(-1, 1)).flatten()\n",
    "y_test = qt_amount.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Save the transformers for later use\n",
    "with open(\"qt_amount.pkl\", \"wb\") as f:\n",
    "    pickle.dump(qt_amount, f)\n",
    "with open(\"qt_size.pkl\", \"wb\") as f:\n",
    "    pickle.dump(qt_size, f)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training set (Base models): {X_train_base.shape}\")\n",
    "print(f\"Validation set (Meta-learner): {X_val_meta.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbb5859e-417e-4877-9a6f-0f59416eca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params =  {'n_estimators': 361, 'max_depth': 10, \n",
    "             'learning_rate': 0.24829236737227453, 'subsample': 0.875390416499723}\n",
    "rf_params = {'n_estimators': 111, 'max_depth': 20, 'min_samples_split': 11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11c3878f-23ec-4fa6-ba72-e50fb2f8a626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n",
      "XGBoost training completed.\n",
      "Training Random Forest model...\n",
      "Random Forest training completed.\n",
      "XGBoost Validation Score: 0.9182581323288751\n",
      "Random Forest Validation Score: 0.918175433734719\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define models with the best parameters\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=xgb_params['n_estimators'],\n",
    "    max_depth=xgb_params['max_depth'],\n",
    "    learning_rate=xgb_params['learning_rate'],\n",
    "    subsample=xgb_params['subsample'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=rf_params['n_estimators'],\n",
    "    max_depth=rf_params['max_depth'],\n",
    "    min_samples_split=rf_params['min_samples_split'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train XGBoost model\n",
    "print(\"Training XGBoost model...\")\n",
    "xgb_model.fit(X_train_base, y_train_base)\n",
    "print(\"XGBoost training completed.\")\n",
    "\n",
    "# Train Random Forest model\n",
    "print(\"Training Random Forest model...\")\n",
    "rf_model.fit(X_train_base, y_train_base)\n",
    "print(\"Random Forest training completed.\")\n",
    "\n",
    "# Evaluate models on the validation set\n",
    "xgb_val_score = xgb_model.score(X_val_meta, y_val_meta)\n",
    "rf_val_score = rf_model.score(X_val_meta, y_val_meta)\n",
    "\n",
    "print(f\"XGBoost Validation Score: {xgb_val_score}\")\n",
    "print(f\"Random Forest Validation Score: {rf_val_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06e7d4c-2536-4175-8285-479fc1c32a49",
   "metadata": {},
   "source": [
    "# Check if normalizing could improve the performance of optimized rf and xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d0cce8a-7dcb-4c61-a7d2-979ef2ca78f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (Base models): (97683, 51)\n",
      "Validation set (Meta-learner): (32561, 51)\n",
      "Test set: (32562, 51)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "no_missing_merged_loc = pd.read_csv('clean_for_training.csv')\n",
    "no_missing_merged_loc.drop(columns='Unnamed: 0', inplace=True)\n",
    "# Updated list of columns to normalize\n",
    "columns_to_normalize = [\n",
    "    \"rooms_en_imputed\", \"project_count\", \"landmark_count\", \"metro_count\", \n",
    "    \"mall_count\", \"Al Makhtoum International Airport\", \"Burj Al Arab\", \n",
    "    \"Burj Khalifa\", \"City Centre Mirdif\", \"Downtown Dubai\", \n",
    "    \"Dubai International Airport\", \"Dubai Mall\", \"Dubai Parks and Resorts\", \n",
    "    \"Expo 2020 Site\", \"Global Village\", \"Hamdan Sports Complex\", \n",
    "    \"IMG World Adventures\", \"Ibn-e-Battuta Mall\", \"Jabel Ali\", \n",
    "    \"Mall of the Emirates\", \"Marina Mall\", \"Motor City\", \"center\", \n",
    "    \"east\", \"north\", \"south\", \"west\", \n",
    "    \"transaction_datetime_month\", \"transaction_datetime_day\", \n",
    "    \"transaction_datetime_weekday\", \"transaction_datetime_dayofyear\", \n",
    "    \"req_from_month\", \"req_from_weekday\", \"req_from_dayofyear\", \n",
    "    \"req_to_month\", \"req_to_day\", \"req_to_weekday\", \"req_to_dayofyear\",\n",
    "    \"parking_count\"\n",
    "]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply normalization to the specified columns\n",
    "no_missing_merged_loc[columns_to_normalize] = scaler.fit_transform(no_missing_merged_loc[columns_to_normalize])\n",
    "# Define features (X) and target (y)\n",
    "X = no_missing_merged_loc.drop(columns=['amount'])  # Replace 'amount' with your target column if different\n",
    "y = no_missing_merged_loc['amount']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split training data for meta-learner (optional)\n",
    "X_train_base, X_val_meta, y_train_base, y_val_meta = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize QuantileTransformers\n",
    "qt_amount = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "qt_size = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "\n",
    "# Fit transformers on training data\n",
    "# Fit qt_size on transaction and property sizes\n",
    "qt_size.fit(X_train[['transaction_size_sqm', 'property_size_sqm']])\n",
    "\n",
    "# Fit qt_amount on the entire `y_train` dataset\n",
    "qt_amount.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Transform sizes in the training, validation, and test sets\n",
    "X_train_base[['transaction_size_sqm', 'property_size_sqm']] = qt_size.transform(\n",
    "    X_train_base[['transaction_size_sqm', 'property_size_sqm']]\n",
    ")\n",
    "X_val_meta[['transaction_size_sqm', 'property_size_sqm']] = qt_size.transform(\n",
    "    X_val_meta[['transaction_size_sqm', 'property_size_sqm']]\n",
    ")\n",
    "X_test[['transaction_size_sqm', 'property_size_sqm']] = qt_size.transform(\n",
    "    X_test[['transaction_size_sqm', 'property_size_sqm']]\n",
    ")\n",
    "\n",
    "# Transform target variable in the training, validation, and test sets\n",
    "y_train_base = qt_amount.transform(y_train_base.values.reshape(-1, 1)).flatten()\n",
    "y_val_meta = qt_amount.transform(y_val_meta.values.reshape(-1, 1)).flatten()\n",
    "y_test = qt_amount.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training set (Base models): {X_train_base.shape}\")\n",
    "print(f\"Validation set (Meta-learner): {X_val_meta.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a03ee26c-1916-4444-bdf5-084932658810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-26 02:19:06,639] A new study created in memory with name: no-name-8fb4eef5-1134-4d88-b499-63ea5f895d98\n",
      "/home/alkhaldieid/miniforge3/envs/cup/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [02:19:07] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1732150600463/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-11-26 02:19:07,927] Trial 0 finished with value: 0.8148735696333368 and parameters: {'n_estimators': 104, 'max_depth': 6, 'learning_rate': 0.016306712871257366, 'subsample': 0.9500015214084825}. Best is trial 0 with value: 0.8148735696333368.\n",
      "[I 2024-11-26 02:19:10,008] Trial 1 finished with value: 0.8661193599197827 and parameters: {'n_estimators': 463, 'max_depth': 4, 'learning_rate': 0.019468165498085197, 'subsample': 0.796731400630611}. Best is trial 1 with value: 0.8661193599197827.\n",
      "[I 2024-11-26 02:19:11,521] Trial 2 finished with value: 0.8896094540727048 and parameters: {'n_estimators': 423, 'max_depth': 3, 'learning_rate': 0.21295949721066856, 'subsample': 0.9097634240412253}. Best is trial 2 with value: 0.8896094540727048.\n",
      "[I 2024-11-26 02:19:13,460] Trial 3 finished with value: 0.9033981097720515 and parameters: {'n_estimators': 239, 'max_depth': 6, 'learning_rate': 0.10226560787404627, 'subsample': 0.674869704076738}. Best is trial 3 with value: 0.9033981097720515.\n",
      "[I 2024-11-26 02:19:17,714] Trial 4 finished with value: 0.9112295194419868 and parameters: {'n_estimators': 102, 'max_depth': 10, 'learning_rate': 0.06072800949883388, 'subsample': 0.6581751213839135}. Best is trial 4 with value: 0.9112295194419868.\n",
      "[I 2024-11-26 02:19:24,980] Trial 5 finished with value: 0.9138092711756745 and parameters: {'n_estimators': 306, 'max_depth': 9, 'learning_rate': 0.10835522493130668, 'subsample': 0.7368055738251874}. Best is trial 5 with value: 0.9138092711756745.\n",
      "[I 2024-11-26 02:19:28,329] Trial 6 finished with value: 0.887961335149316 and parameters: {'n_estimators': 390, 'max_depth': 6, 'learning_rate': 0.016137978612940845, 'subsample': 0.6689167249840758}. Best is trial 5 with value: 0.9138092711756745.\n",
      "[I 2024-11-26 02:19:41,058] Trial 7 finished with value: 0.9133948605904253 and parameters: {'n_estimators': 151, 'max_depth': 12, 'learning_rate': 0.09900389685998541, 'subsample': 0.9292667857833907}. Best is trial 5 with value: 0.9138092711756745.\n",
      "[I 2024-11-26 02:19:48,133] Trial 8 finished with value: 0.9127436250328423 and parameters: {'n_estimators': 298, 'max_depth': 9, 'learning_rate': 0.04554710387720809, 'subsample': 0.9028967707091949}. Best is trial 5 with value: 0.9138092711756745.\n",
      "[I 2024-11-26 02:19:58,945] Trial 9 finished with value: 0.9133864007876724 and parameters: {'n_estimators': 136, 'max_depth': 12, 'learning_rate': 0.09945640021760606, 'subsample': 0.6047023156216121}. Best is trial 5 with value: 0.9138092711756745.\n",
      "[I 2024-11-26 02:20:06,874] Trial 10 finished with value: 0.904915731147642 and parameters: {'n_estimators': 323, 'max_depth': 9, 'learning_rate': 0.2996115455682617, 'subsample': 0.7836237166907685}. Best is trial 5 with value: 0.9138092711756745.\n",
      "[I 2024-11-26 02:20:25,302] Trial 11 finished with value: 0.9119872618984296 and parameters: {'n_estimators': 224, 'max_depth': 12, 'learning_rate': 0.12258308838661619, 'subsample': 0.8471592340686266}. Best is trial 5 with value: 0.9138092711756745.\n",
      "[I 2024-11-26 02:20:32,994] Trial 12 finished with value: 0.9128236219372129 and parameters: {'n_estimators': 192, 'max_depth': 10, 'learning_rate': 0.039711192194360166, 'subsample': 0.728789328563387}. Best is trial 5 with value: 0.9138092711756745.\n",
      "[I 2024-11-26 02:20:47,779] Trial 13 finished with value: 0.9114210605846765 and parameters: {'n_estimators': 314, 'max_depth': 11, 'learning_rate': 0.16497654038544532, 'subsample': 0.999584478906995}. Best is trial 5 with value: 0.9138092711756745.\n",
      "[I 2024-11-26 02:20:53,329] Trial 14 finished with value: 0.9130828601640468 and parameters: {'n_estimators': 358, 'max_depth': 8, 'learning_rate': 0.07630261288688536, 'subsample': 0.74433251056583}. Best is trial 5 with value: 0.9138092711756745.\n",
      "[I 2024-11-26 02:20:58,296] Trial 15 finished with value: 0.9060236019386605 and parameters: {'n_estimators': 272, 'max_depth': 8, 'learning_rate': 0.026944275024876476, 'subsample': 0.8516854262962561}. Best is trial 5 with value: 0.9138092711756745.\n",
      "[I 2024-11-26 02:21:07,310] Trial 16 finished with value: 0.9125162106453032 and parameters: {'n_estimators': 164, 'max_depth': 11, 'learning_rate': 0.15755067119329652, 'subsample': 0.8593744927480239}. Best is trial 5 with value: 0.9138092711756745.\n",
      "[I 2024-11-26 02:21:10,093] Trial 17 finished with value: 0.906777133847482 and parameters: {'n_estimators': 247, 'max_depth': 7, 'learning_rate': 0.0710779680173849, 'subsample': 0.7381049661732392}. Best is trial 5 with value: 0.9138092711756745.\n",
      "[I 2024-11-26 02:21:26,844] Trial 18 finished with value: 0.9067147808925317 and parameters: {'n_estimators': 492, 'max_depth': 10, 'learning_rate': 0.2934185601018412, 'subsample': 0.9727378089422152}. Best is trial 5 with value: 0.9138092711756745.\n",
      "[I 2024-11-26 02:21:46,388] Trial 19 finished with value: 0.9141857739508888 and parameters: {'n_estimators': 197, 'max_depth': 12, 'learning_rate': 0.035707771528736075, 'subsample': 0.9090060296096989}. Best is trial 19 with value: 0.9141857739508888.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'n_estimators': 197, 'max_depth': 12, 'learning_rate': 0.035707771528736075, 'subsample': 0.9090060296096989}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import numpy as np\n",
    "import cupy \n",
    "\n",
    "def optimize_xgboost(trial, X_train, y_train):\n",
    "    # Define the hyperparameters to tune\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"tree_method\": \"hist\",  # Use \"hist\" for GPU\n",
    "        \"device\": \"cuda\"  # Specify CUDA device for GPU training\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**param)\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=kf, scoring=\"r2\")\n",
    "    return np.mean(scores)\n",
    "\n",
    "def optimize_model(optimize_func, X_train, y_train, n_trials=20):\n",
    "    # Convert CuPy arrays to NumPy\n",
    "    X_train = X_train.get()\n",
    "    y_train = y_train.get()\n",
    "    \n",
    "    def objective(trial):\n",
    "        return optimize_func(trial, X_train, y_train)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params\n",
    "\n",
    "# Usage\n",
    "X_train = cupy.array(X_train_base)\n",
    "y_train = cupy.array(y_train_base)\n",
    "\n",
    "best_params_xgb = optimize_model(optimize_xgboost, X_train, y_train)\n",
    "print(\"Best parameters for XGBoost:\", best_params_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62ff258d-e04b-4355-b332-0f5bd8974e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing Random Forest...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "NDFrame.get() missing 1 required positional argument: 'key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(scores)  \u001b[38;5;66;03m# Return the average R2 score\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizing Random Forest...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m best_params_rf \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimize_random_forest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_base\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters for Random Forest:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_params_rf)\n",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m(optimize_func, X_train, y_train, n_trials)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize_model\u001b[39m(optimize_func, X_train, y_train, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Convert CuPy arrays to NumPy\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(trial):\n",
      "\u001b[0;31mTypeError\u001b[0m: NDFrame.get() missing 1 required positional argument: 'key'"
     ]
    }
   ],
   "source": [
    "# Define Random Forest optimization with cross-validation\n",
    "def optimize_random_forest(trial, X_train, y_train):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"n_jobs\": -1  # Enable parallel processing\n",
    "    }\n",
    "    model = RandomForestRegressor(**params, random_state=42)\n",
    "\n",
    "    # 5-fold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=kf, scoring=\"r2\", n_jobs=-1)\n",
    "    return np.mean(scores)  # Return the average R2 score\n",
    "\n",
    "print(\"Optimizing Random Forest...\")\n",
    "best_params_rf = optimize_model(optimize_random_forest, X_train_base, y_train_base)\n",
    "print(\"Best parameters for Random Forest:\", best_params_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8474e8ad-19f8-4f36-8835-9b75f5489b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
